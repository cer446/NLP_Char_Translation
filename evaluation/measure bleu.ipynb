{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'bleu.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "run -i bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "#%%\n",
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/test_source.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    source = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        source.append(line[0])\n",
    "        \n",
    "  \n",
    "    \n",
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/test_target.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    targets = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        targets.append(line[0])\n",
    "        \n",
    "        \n",
    "    \n",
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/test_pred2.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    predictions = []\n",
    "    s2=[]\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        s1=(line.split('  '))\n",
    "        #print(s1)\n",
    "        for k, word in enumerate(s1):\n",
    "            s2.append(\" \".join([\"\".join(w.split(\" \")) for w in word.split(\"  \")]))\n",
    "        predictions.append(\" \".join(s2))\n",
    "        s2=[]\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for the QRNN\n",
    "\n",
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/predicted_CNN.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    pred_cnn = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        pred_cnn.append(line[0])\n",
    "        \n",
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/target_CNN.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    target_cnn = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        target_cnn.append(line[0])\n",
    "        \n",
    "        \n",
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/source_CNN.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    source_cnn = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        source_cnn.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \", s) #separates punctuation from the word\n",
    "    #s = re.sub(r\"([.!?])\", r\" \\1\", s) #separates punctuation from the word\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) #strips anything that isn't a character of punctuation\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of /n \n",
    "source_u=[]\n",
    "for i, word in enumerate(source): \n",
    "    source_u.append(normalizeString(word.strip()))\n",
    "    \n",
    "target_u=[]\n",
    "for i, t in enumerate(targets):\n",
    "    target_u.append(normalizeString(t.strip()))\n",
    "    \n",
    "pred_u=[]\n",
    "for i, p in enumerate(predictions):\n",
    "    pred_u.append(normalizeString(p.strip()))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for cnn \n",
    "pred_u1=[]\n",
    "for i, p in enumerate(pred_cnn):\n",
    "    pred_u1.append(normalizeString(p.strip()))  \n",
    "\n",
    "target_u1=[]\n",
    "for i, p in enumerate(target_cnn):\n",
    "    target_u1.append(normalizeString(p.strip()))  \n",
    "\n",
    "source_u1=[]\n",
    "for i, p in enumerate(source_cnn):\n",
    "    source_u1.append(normalizeString(p.strip()))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624\n",
      "2624\n",
      "2624\n"
     ]
    }
   ],
   "source": [
    "print(len(source_u1))\n",
    "print(len(target_u1))\n",
    "print(len(pred_u1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_u1=source_u1[0:2624]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we type we file we go to dinner it s fine \n",
      "\n",
      "we typing them we re eating that is okay that s okay \n"
     ]
    }
   ],
   "source": [
    "print(target_u[800])\n",
    "print('')\n",
    "print(pred_u[800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.49\n"
     ]
    }
   ],
   "source": [
    "#bleu score\n",
    "b=moses_multi_bleu(pred_u1, target_u1, lowercase=False)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/compounds.txt\", \"r\", encoding=\"utf8\") as comp:\n",
    "    compound_words = []\n",
    "    \n",
    "    for i,line in enumerate(comp):        \n",
    "        compound_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment-1: find the sentences where the compound words occurs and  \n",
    "#find the  bleu score for those\n",
    "pred_comp=[]\n",
    "target_comp=[]\n",
    "\n",
    "for i in range(len(pred_u1)):\n",
    "    for w in source_u1[i].split():\n",
    "        if w in compound_words:\n",
    "            #print(w)\n",
    "            #print(i)\n",
    "            pred_comp.append(pred_u1[i])\n",
    "            target_comp.append(target_u1[i])\n",
    "            break\n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weil wir keine gene herumschieben wollen \n",
      "\n",
      "because we don t want to move genes around \n",
      "\n",
      "because we want to shift around the way we want to do that \n"
     ]
    }
   ],
   "source": [
    "print(source_u[949]) \n",
    "print('')\n",
    "print(target_u[949 ]) \n",
    "print('')\n",
    "print(pred_u[949])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932\n"
     ]
    }
   ],
   "source": [
    "#find when qrnn output is given . \n",
    "ind=1\n",
    "for w in target_u1:\n",
    "    ind=ind+1\n",
    "    if w=='because we don t want to move genes around ':\n",
    "        print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it s not the too e n t i n t i tis ic t ic t ic t ic '"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_u1[931]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "2.54\n"
     ]
    }
   ],
   "source": [
    "blue_comp=moses_multi_bleu(pred_comp, target_comp, lowercase=False)\n",
    "\n",
    "print(blue_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.420654911838792\n",
      "19.2191435768262\n",
      "16.17632241813602\n"
     ]
    }
   ],
   "source": [
    "#experiment-2: find the sentences where the compound words occurs and \n",
    "#count the number of words in each sentence.\n",
    "target_length=[]\n",
    "source_length=[]\n",
    "pred_length=[]\n",
    "\n",
    "t=[]\n",
    "s=[]\n",
    "p=[]\n",
    "\n",
    "for i in range(len(pred_u1)):\n",
    "    for w in source_u1[i].split():\n",
    "        if w in compound_words:\n",
    "            target_length.append(len(target_u1[i].split()))\n",
    "            source_length.append(len(source_u1[i].split())) \n",
    "            pred_length.append(len(pred_u1[i].split())) \n",
    "\n",
    "            t.append(target_u1[i])\n",
    "            s.append(source_u1[i])\n",
    "            p.append(pred_u1[i])\n",
    "\n",
    "            break\n",
    "         \n",
    "\n",
    "print(sum(target_length) / float(len(target_length)))\n",
    "print(sum(source_length) / float(len(source_length)))\n",
    "print(sum(pred_length) / float(len(pred_length)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.552193702963\n",
      "0.500971847087\n",
      "0.61058925154\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats \n",
    "\n",
    "t_l = np.array(target_length)\n",
    "s_l = np.array(source_length)\n",
    "p_l = np.array(pred_length)\n",
    "\n",
    "print(stats.sem(t_l))\n",
    "print(stats.sem(s_l))\n",
    "print(stats.sem(p_l))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not using this anymore\n",
    "\n",
    "target_noncomp_length=[]\n",
    "source_noncomp_length=[]\n",
    "pred_noncomp_length=[]\n",
    "\n",
    "#find the non compound sentences \n",
    "for i in range(len(target_u)):\n",
    "    for sentence in target_u:\n",
    "        if sentence in t:\n",
    "            break\n",
    "        else:\n",
    "            target_noncomp_length.append(len(target_u[i].split()))\n",
    "        \n",
    "for i in range(len(source_u)):\n",
    "    for sentence in source_u:\n",
    "        if sentence in s:\n",
    "            break\n",
    "        else:\n",
    "            source_noncomp_length.append(len(source_u[i].split()))\n",
    "            \n",
    "for i in range(len(pred_u)):\n",
    "    for sentence in pred_u:\n",
    "        if sentence in s:\n",
    "            break\n",
    "        else:\n",
    "            pred_noncomp_length.append(len(pred_u[i].split()))\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum(target_noncomp_length) / float(len(target_noncomp_length)))\n",
    "print(sum(source_noncomp_length) / float(len(source_noncomp_length)))\n",
    "print(sum(pred_noncomp_length) / float(len(pred_noncomp_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.036966463414634\n",
      "15.754954268292684\n",
      "14.690167682926829\n",
      "0.201143566814\n",
      "0.178747859663\n",
      "0.217527359537\n"
     ]
    }
   ],
   "source": [
    "#overall length\n",
    "overall_source_length=[]\n",
    "overall_target_length=[]\n",
    "overall_pred_length=[]\n",
    "\n",
    "\n",
    "for i in range(len(source_u1)):\n",
    "    overall_source_length.append(len(source_u1[i].split()))\n",
    "    overall_target_length.append(len(target_u1[i].split()))\n",
    "    overall_pred_length.append(len(pred_u1[i].split()))\n",
    "\n",
    "            \n",
    "print(sum(overall_source_length) / float(len(overall_source_length)))\n",
    "print(sum(overall_target_length) / float(len(overall_target_length)))\n",
    "print(sum(overall_pred_length) / float(len(overall_pred_length)))\n",
    "\n",
    "t_l = np.array(overall_target_length)\n",
    "s_l = np.array(overall_source_length)\n",
    "p_l = np.array(overall_pred_length)\n",
    "\n",
    "print(stats.sem(t_l))\n",
    "print(stats.sem(s_l))\n",
    "print(stats.sem(p_l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare long vs short sentences\n",
    "#use 10 as cutoff\n",
    "\n",
    "long_sent_pred=[]\n",
    "long_sent_target=[]\n",
    "short_sent_pred=[]\n",
    "short_sent_target=[]\n",
    "\n",
    "for i in range(len(pred_u1)):\n",
    "    if len(source_u1[i])>100:\n",
    "        long_sent_pred.append(pred_u1[i])\n",
    "        long_sent_target.append(target_u1[i])\n",
    "    else:\n",
    "        short_sent_pred.append(pred_u1[i])\n",
    "        short_sent_target.append(target_u1[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.93\n",
      "2.89\n"
     ]
    }
   ],
   "source": [
    "blue_long=moses_multi_bleu(long_sent_pred, long_sent_target, lowercase=False)\n",
    "print(blue_long) \n",
    "\n",
    "blue_short=moses_multi_bleu(short_sent_pred, short_sent_target, lowercase=False)\n",
    "print(blue_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
