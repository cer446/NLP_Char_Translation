{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bilals01/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "run -i bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "import re\n",
    "import random\n",
    "import collections\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "#%%\n",
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/source_valid.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    source = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        source.append(line[0])\n",
    "        \n",
    "  \n",
    "    \n",
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/target_valid.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    targets = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        targets.append(line[0])\n",
    "        \n",
    "        \n",
    "    \n",
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/pred_valid_m1.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    predictions1 = []\n",
    "    s2=[]\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        s1=(line.split('  '))\n",
    "        #print(s1)\n",
    "        for k, word in enumerate(s1):\n",
    "            s2.append(\" \".join([\"\".join(w.split(\" \")) for w in word.split(\"  \")]))\n",
    "        predictions1.append(\" \".join(s2))\n",
    "        s2=[]\n",
    "        \n",
    "        \n",
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/pred_valid_m2.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    predictions2 = []\n",
    "    s2=[]\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        s1=(line.split('  '))\n",
    "        #print(s1)\n",
    "        for k, word in enumerate(s1):\n",
    "            s2.append(\" \".join([\"\".join(w.split(\" \")) for w in word.split(\"  \")]))\n",
    "        predictions2.append(\" \".join(s2))\n",
    "        s2=[]\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \", s) #separates punctuation from the word\n",
    "    #s = re.sub(r\"([.!?])\", r\" \\1\", s) #separates punctuation from the word\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) #strips anything that isn't a character of punctuation\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get rid of /n \n",
    "source_u=[]\n",
    "for i, word in enumerate(source): \n",
    "    source_u.append(normalizeString(word.strip()))\n",
    "    \n",
    "target_u=[]\n",
    "for i, t in enumerate(targets):\n",
    "    target_u.append(normalizeString(t.strip()))\n",
    "    \n",
    "pred_u1=[]\n",
    "for i, p in enumerate(predictions1):\n",
    "    pred_u1.append(normalizeString(p.strip()))  \n",
    "\n",
    "pred_u2=[]\n",
    "for i, p in enumerate(predictions2):\n",
    "    pred_u2.append(normalizeString(p.strip()))  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2052\n",
      "2052\n",
      "2052\n",
      "2052\n"
     ]
    }
   ],
   "source": [
    "print(len(source))\n",
    "print(len(targets))\n",
    "print(len(predictions1))\n",
    "print(len(predictions2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and so she went to dance every year on the first of may and danced until late and probably drank a glass of punch too much once and ended up falling and broke you can see it here on the right side her leg her femoral neck \n",
      "\n",
      "and so she is she was the first may first go to the dance and dancing and probably once a glass of bowl and probably once a glass of bowl that you see the leg broke the ginger that you see the leg broke the ginger that you see the leg broke the ginger \n"
     ]
    }
   ],
   "source": [
    "print(target_u[8950])\n",
    "print('')\n",
    "print(pred_u[8950])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.39\n",
      "13.5\n"
     ]
    }
   ],
   "source": [
    "#bleu score\n",
    "a=moses_multi_bleu(pred_u1, target_u, lowercase=False)\n",
    "b=moses_multi_bleu(pred_u2, target_u, lowercase=False)\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/bilals01/Documents/NLP/project/quasi-rnn-run-12-09/evaluation/compounds.txt\", \"r\", encoding=\"utf8\") as comp:\n",
    "    compound_words = []\n",
    "    \n",
    "    for i,line in enumerate(comp):        \n",
    "        compound_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#experiment-1: find the sentences where the compound words occurs and  \n",
    "#find the  bleu score for those\n",
    "pred_comp=[]\n",
    "target_comp=[]\n",
    "\n",
    "for i in range(len(pred_u)):\n",
    "    for w in source_u[i].split():\n",
    "        if w in compound_words:\n",
    "            pred_comp.append(pred_u[i])\n",
    "            target_comp.append(target_u[i])\n",
    "            break\n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "blue_comp=moses_multi_bleu(pred_comp, target_comp, lowercase=False)\n",
    "\n",
    "print(blue_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.807193396226417\n",
      "23.28183962264151\n",
      "28.77063679245283\n"
     ]
    }
   ],
   "source": [
    "#experiment-2: find the sentences where the compound words occurs and \n",
    "#count the number of words in each sentence.\n",
    "target_length=[]\n",
    "source_length=[]\n",
    "pred_length=[]\n",
    "\n",
    "t=[]\n",
    "s=[]\n",
    "p=[]\n",
    "\n",
    "for i in range(len(pred_u)):\n",
    "    for w in source_u[i].split():\n",
    "        if w in compound_words:\n",
    "            target_length.append(len(target_u[i].split()))\n",
    "            source_length.append(len(source_u[i].split())) \n",
    "            pred_length.append(len(pred_u[i].split())) \n",
    "\n",
    "            t.append(target_u[i])\n",
    "            s.append(source_u[i])\n",
    "            p.append(pred_u[i])\n",
    "\n",
    "            break\n",
    "         \n",
    "\n",
    "print(sum(target_length) / float(len(target_length)))\n",
    "print(sum(source_length) / float(len(source_length)))\n",
    "print(sum(pred_length) / float(len(pred_length)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.353135948518\n",
      "0.346535100124\n",
      "0.376896965315\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats \n",
    "\n",
    "t_l = np.array(target_length)\n",
    "s_l = np.array(source_length)\n",
    "p_l = np.array(pred_length)\n",
    "\n",
    "print(stats.sem(t_l))\n",
    "print(stats.sem(s_l))\n",
    "print(stats.sem(p_l))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#not using this anymore\n",
    "\n",
    "target_noncomp_length=[]\n",
    "source_noncomp_length=[]\n",
    "pred_noncomp_length=[]\n",
    "\n",
    "#find the non compound sentences \n",
    "for i in range(len(target_u)):\n",
    "    for sentence in target_u:\n",
    "        if sentence in t:\n",
    "            break\n",
    "        else:\n",
    "            target_noncomp_length.append(len(target_u[i].split()))\n",
    "        \n",
    "for i in range(len(source_u)):\n",
    "    for sentence in source_u:\n",
    "        if sentence in s:\n",
    "            break\n",
    "        else:\n",
    "            source_noncomp_length.append(len(source_u[i].split()))\n",
    "            \n",
    "for i in range(len(pred_u)):\n",
    "    for sentence in pred_u:\n",
    "        if sentence in s:\n",
    "            break\n",
    "        else:\n",
    "            pred_noncomp_length.append(len(pred_u[i].split()))\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sum(target_noncomp_length) / float(len(target_noncomp_length)))\n",
    "print(sum(source_noncomp_length) / float(len(source_noncomp_length)))\n",
    "print(sum(pred_noncomp_length) / float(len(pred_noncomp_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.182850711142944\n",
      "16.691190013301956\n",
      "20.52174357924895\n",
      "0.119243033233\n",
      "0.117186928627\n",
      "0.144922862633\n"
     ]
    }
   ],
   "source": [
    "#overall length\n",
    "overall_source_length=[]\n",
    "overall_target_length=[]\n",
    "overall_pred_length=[]\n",
    "\n",
    "\n",
    "for i in range(len(source_u)):\n",
    "    overall_source_length.append(len(source_u[i].split()))\n",
    "    overall_target_length.append(len(target_u[i].split()))\n",
    "    overall_pred_length.append(len(pred_u[i].split()))\n",
    "\n",
    "            \n",
    "print(sum(overall_source_length) / float(len(overall_source_length)))\n",
    "print(sum(overall_target_length) / float(len(overall_target_length)))\n",
    "print(sum(overall_pred_length) / float(len(overall_pred_length)))\n",
    "\n",
    "t_l = np.array(overall_target_length)\n",
    "s_l = np.array(overall_source_length)\n",
    "p_l = np.array(overall_pred_length)\n",
    "\n",
    "print(stats.sem(t_l))\n",
    "print(stats.sem(s_l))\n",
    "print(stats.sem(p_l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compare long vs short sentences\n",
    "#use 10 as cutoff\n",
    "\n",
    "long_sent_pred=[]\n",
    "long_sent_target=[]\n",
    "short_sent_pred=[]\n",
    "short_sent_target=[]\n",
    "\n",
    "for i in range(len(source_u)):\n",
    "    if len(source_u[i])>100:\n",
    "        long_sent_pred.append(pred_u[i])\n",
    "        long_sent_target.append(target_u[i])\n",
    "    else:\n",
    "        short_sent_pred.append(pred_u[i])\n",
    "        short_sent_target.append(target_u[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4\n",
      "13.79\n"
     ]
    }
   ],
   "source": [
    "blue_long=moses_multi_bleu(long_sent_pred, long_sent_target, lowercase=False)\n",
    "print(blue_long) \n",
    "\n",
    "blue_short=moses_multi_bleu(short_sent_pred, short_sent_target, lowercase=False)\n",
    "print(blue_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
