{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovered Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#targets\n",
    "\n",
    "with open(\"/Users/carolineroper/Documents/School/Natural Language Processing/quasi-rnn-original/dev_target_seqs.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    targets = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        targets.append(line[0])\n",
    "        \n",
    "#predictions        \n",
    "    \n",
    "with open(\"/Users/carolineroper/Documents/School/Natural Language Processing/quasi-rnn-original/decoded_original.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    predictions = []\n",
    "    s2=[]\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        s1=(line.split('  '))\n",
    "        #print(s1)\n",
    "        for k, word in enumerate(s1):\n",
    "            s2.append(\" \".join([\"\".join(w.split(\" \")) for w in word.split(\"  \")]))\n",
    "        predictions.append(\" \".join(s2))\n",
    "        s2=[]\n",
    "        \n",
    "#training targets\n",
    "        \n",
    "with open(\"/Users/carolineroper/Documents/School/Natural Language Processing/quasi-rnn-original/train_target_seqs.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    \n",
    "    train_targets = []\n",
    "    \n",
    "    for i, line in enumerate(f):\n",
    "        \n",
    "        line = line.split('<JOIN>')\n",
    "        train_targets.append(line[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randoms = random.sample(range(0,len(targets)), 30)\n",
    "\n",
    "for i in randoms:\n",
    "    print (targets[i], predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Vocabulary of Training Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) #separates punctuation from the word\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s) #strips anything that isn't a character of punctuation\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_vocabulary(seq, vocab_size):\n",
    "    counter = Counter()\n",
    "    for sentence in seq:\n",
    "        counter.update(normalizeString(sentence).split())\n",
    "    vocabulary = [count[0] for count in counter.most_common(vocab_size)]\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vocab = find_vocabulary(train_targets, 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49545"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_vocab = find_vocabulary(predictions, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3874"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get \"Discovered\" Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oov = []\n",
    "\n",
    "for word in result_vocab:\n",
    "    if word not in train_vocab:\n",
    "        oov.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_example(word):\n",
    "    for i in range(len(predictions)):\n",
    "        if word in normalizeString(predictions[i]).split():\n",
    "            index_where_found = i\n",
    "    if word in normalizeString(targets[index_where_found]).split():\n",
    "        correctness = True\n",
    "    else:\n",
    "        correctness= False\n",
    "    return word, targets[index_where_found], predictions[index_where_found], correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ethnosphere',\n",
       " \"And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\\n\",\n",
       " 'And we take a series of rices in the ethnosphere, on the stage where we should be our audience to place that they were taking the public of space and hopefully they can take a food that the world is about the central office that the central offers that we can take a food in the wisdom of all people \\n',\n",
       " True)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_example('ethnosphere')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Find Correct \"Discovered Words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = 0\n",
    "correct_discoveries = []\n",
    "disc_targets = []\n",
    "disc_predictions = []\n",
    "\n",
    "for word in oov:\n",
    "    word, target, prediction, correctness = find_example(word)\n",
    "    if correctness == True:\n",
    "        num_correct += 1\n",
    "        correct_discoveries.append(word)\n",
    "        disc_targets.append(target)\n",
    "        disc_predictions.append(prediction)\n",
    "        \n",
    "num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ethnosphere',\n",
       " 'burk',\n",
       " 'monoamine',\n",
       " 'bioland',\n",
       " 'frosty',\n",
       " 'barasana',\n",
       " 'marta',\n",
       " 'yanomami',\n",
       " 'snavely',\n",
       " 'pareto',\n",
       " 'playoff',\n",
       " 'katey',\n",
       " 'degger',\n",
       " 'hackteria',\n",
       " 'profilers',\n",
       " 'imploded',\n",
       " 'ude',\n",
       " 'lento',\n",
       " 'buridan',\n",
       " 'solon']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_discoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#many of these are proper nouns or scientific terms, but a handful can be attributed to correctly\n",
    "#compounding words, such as \"profilers\" \"playoff\" \"ethnosphere\" and \"bioland\", \"imploded\" perhaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find examples of Incorrect \"Discovered Words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incorrect_oov = list(set(oov).difference(set(correct_discoveries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thefile = open('incorrect_new_words.txt', 'w', encoding = 'utf-8')\n",
    "\n",
    "randoms = random.sample(range(0,len(incorrect_oov)), 10)\n",
    "\n",
    "for i in randoms:\n",
    "    output = list(find_example(incorrect_oov[i]))\n",
    "    #output = [str(x) for x in output]\n",
    "    thefile.write(\"\\n\")\n",
    "    for x in output[0:3]:\n",
    "        thefile.write(\"\\n%s\" % x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
